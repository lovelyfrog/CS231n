{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文介绍了pytorch 的一些基础知识，比如 tensors，operations等，还有pytorch 的自动微分机制  \n",
    "本文参考自[DEEP LEARNING WITH PYTORCH: A 60 MINUTE BLITZ](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch是什么"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch 是一个科学计算包：\n",
    "* numpy 的一个替代（可以使用GPU）\n",
    "* 深度学习框架提供了最大限度的自由和速度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensors 类似于 numpy 里的 ndarrays，除此之外它可以在 GPU 中加速运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建一个未初始化 5x3 的矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000e+00, 8.5899e+09, 0.0000e+00],\n",
      "        [8.5899e+09, 5.6052e-45, 1.4714e-43],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [1.6956e-43, 0.0000e+00, 4.5318e-22]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(5,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建一个随机初始化矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5588, 0.6994, 0.2045],\n",
      "        [0.4634, 0.0390, 0.9279],\n",
      "        [0.8828, 0.4020, 0.3207],\n",
      "        [0.0759, 0.3997, 0.0225],\n",
      "        [0.5889, 0.9263, 0.1755]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建一个全 0 而且 dtype 为 long 型的矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(5,3,dtype=torch.long)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建一个从直接从数据得到的 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1,2])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[ 0.5565, -3.5219,  1.5671],\n",
      "        [ 0.5087,  0.6783,  0.6356],\n",
      "        [ 0.7736,  1.0188,  0.7899],\n",
      "        [ 0.4047,  1.5271, -0.0099],\n",
      "        [-0.9604, -1.4309,  1.8827]])\n"
     ]
    }
   ],
   "source": [
    "x = x.new_ones(5,3, dtype=torch.double)    #new_* 方法 传入 sizes\n",
    "print(x)\n",
    "x = torch.randn_like(x, dtype=torch.float)  # override dtype\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得到 x 的 sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* note:  \n",
    "torch.size 实际上是一个 tuple 所以支持所有的 tuple 操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "operations 有很多语法，我们先看一下加法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "addition：语法1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0673, -3.4936,  2.1493],\n",
      "        [ 0.8217,  1.6515,  1.5904],\n",
      "        [ 1.2576,  1.8584,  0.9448],\n",
      "        [ 1.1150,  1.9759,  0.3460],\n",
      "        [-0.8035, -0.8520,  1.9764]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.rand(5,3)\n",
    "print(x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "addition：语法2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0673, -3.4936,  2.1493],\n",
      "        [ 0.8217,  1.6515,  1.5904],\n",
      "        [ 1.2576,  1.8584,  0.9448],\n",
      "        [ 1.1150,  1.9759,  0.3460],\n",
      "        [-0.8035, -0.8520,  1.9764]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.add(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "addtion：提供一个output tensor 作为参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0673, -3.4936,  2.1493],\n",
      "        [ 0.8217,  1.6515,  1.5904],\n",
      "        [ 1.2576,  1.8584,  0.9448],\n",
      "        [ 1.1150,  1.9759,  0.3460],\n",
      "        [-0.8035, -0.8520,  1.9764]])\n"
     ]
    }
   ],
   "source": [
    "result = torch.empty(5,3)\n",
    "torch.add(x, y, out=result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "addition：in-place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0673, -3.4936,  2.1493],\n",
      "        [ 0.8217,  1.6515,  1.5904],\n",
      "        [ 1.2576,  1.8584,  0.9448],\n",
      "        [ 1.1150,  1.9759,  0.3460],\n",
      "        [-0.8035, -0.8520,  1.9764]])\n"
     ]
    }
   ],
   "source": [
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "任何在原地(in-place)改变 tensor 的 operations 必须在后面加一个 _ 后缀，例如：x.copy_(y), x.t_() 将会改变 x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以对 tensor 做任何 numy-like 的索引操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-3.5219,  0.6783,  1.0188,  1.5271, -1.4309])\n"
     ]
    }
   ],
   "source": [
    "print(x[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resizing: 如果你想 resize/reshape tensor,可以使用 torch.view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n",
      "tensor([[-0.6481,  0.7575, -0.7781, -0.1815],\n",
      "        [ 0.9559, -0.8391, -0.7656, -1.6857],\n",
      "        [-0.6311, -0.5878, -1.5626,  1.7300],\n",
      "        [-0.1870,  0.3486, -0.5177, -0.6897]])\n",
      "tensor([-0.6481,  0.7575, -0.7781, -0.1815,  0.9559, -0.8391, -0.7656, -1.6857,\n",
      "        -0.6311, -0.5878, -1.5626,  1.7300, -0.1870,  0.3486, -0.5177, -0.6897])\n",
      "tensor([[-0.6481,  0.7575, -0.7781, -0.1815,  0.9559, -0.8391, -0.7656, -1.6857],\n",
      "        [-0.6311, -0.5878, -1.5626,  1.7300, -0.1870,  0.3486, -0.5177, -0.6897]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4,4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1,8)\n",
    "print(x.size(), y.size(), z.size())\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你有一个 只有一个元素的 tensor,可以用 .item() 来得到它的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6449])\n",
      "-0.6449040174484253\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read Later**  \n",
    "[more operations](https://pytorch.org/docs/stable/torch.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy bridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将 torch tensor 转化成 numpy array 非常简单（反过来也是）， 它们共享潜在的内存位置，改变一个也会将另外一个改变"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 将 torch tensor 转化成 numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.numpy()\n",
    "print(b)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看看 numpy array 是怎么改变的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 将 numpy array 转化成 torch tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看看 torch tensor 是怎么改变的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有在 CPU 上运行的 tensor 除了 CharTensor 都支持转化为 numpy 或者相反的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你有 cuda 的话你可以实验一下，但是我没有 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd: 自动微分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "autograd 在 PyTorch 中是一个对神经网络十分重要的包，它能对 tensor 上的所有 operations 自动求梯度。它是一个 define-by-run 的框架，意味着你的反向传播被你的代码如何运行所决定了，并且每次迭代都可能会不同。  \n",
    "让我们从一些例子中了解这件事"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.tensor 是这个包的主要的类，如果你设置 .require_grad 为 True，它就会追踪所有在它上做的 operations。当你结束了所有的计算后，调用 .backward() 就会让所有的梯度自动计算，这个 tensor 的梯度就会加进 .grad 属性中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了停止一个 tensor 追踪历史，可以调用 .detach() 来将它从计算历史中断开，阻止了未来的计算被追踪"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了阻止追踪历史（占用内存），你可以使用 with torch.no_grad(): 包裹代码：当你评估一个模型的时候，因为它可能会有可训练的参数，那么这样就很有帮助，因为我们不需要梯度 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还有一个在 autograd 的实现中很重要的一个类是 Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor 和 Function 是互相连接在一起构成一个非循环图，这个图编码了计算的完整的历史。每个 Tensor 都有一个 .grad_fn 的属性，它引用了创造了这个 Tensor 的 Function（除了用户自己创建的 Tensors，它们的 grad_fn 是 None）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你想计算导数，可以在 Tensor 上调用 .backward() ，如果一个 Tensor 是标量（比如它只有一个元素），你不需要具体说明 backward() 的参数，然而如果它有更多的元素，你需要给 gradient 标明与它的 tensor 相同的 shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面创建一个 tensor 并设置 requires_grad=True 来追踪与它有关的计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "做一个 tensor operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y 是有一个 operation (加法) 创建的，所以它有 grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x11c1ef4a8>\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 y 上做更多的 operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>)\n",
      "tensor(27., grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(z)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".requires_grad(...) 在原地改变一个 Tensor 的 require_grad flag，这个输入的 flag 默认是 False 如果没有给的话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x11c1f85f8>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们开始反向传播，因为 out 只包含一个值，out.backward() 等同于 out.backward(torch.tensor(1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打印 d(out)/dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们有一个函数 $\\vec{y}=f(\\vec{x})$ ，那么 $\\vec{y}$  对于 $\\vec{x}$ 的梯度是一个 Jacobian 矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.autograd 是一个计算 Jacobian-vector 乘积的工具，若 $v$ 是 标量函数 $l=g(\\vec{y})$ 的梯度，也就是 $v = (\\frac{\\partial{l}}{\\partial{y_1}}, ..., \\frac{\\partial{l}}{\\partial{y_m}})$，那么根据链式法则，Jacobian-vector 的乘积就是 $l$ 相对于 $\\vec{x}$ 的梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jacobian-vector 乘积的特性使得给模型喂入外部的梯度变得简单，我们看一下 Jacobian-vector 乘积的一个例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 805.7248, -670.8773,  -32.1244], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "    \n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在 y 不再是一个标量，torch.autograd 就不能直接计算完整的 Jacobian,然而如果我们想得到 Jacobian-vector 乘积，只要传入 vector 到 backward 中就行了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.1200e+01, 5.1200e+02, 5.1200e-02])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(v)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你也可以让 autograd 停止从 .requires_grad=True 的 tensors 追踪历史，通过包裹代码块进 with torch.no_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print(y.requires_grad)\n",
    "print((x * 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x * 2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read Later:**  \n",
    "[autograd 和 Function 的文档](https://pytorch.org/docs/autograd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
